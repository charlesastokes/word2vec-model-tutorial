{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Word2Vec Vectorized Embedding Model Training from Scratch\n",
    "\n",
    "# **Warning: this is a work in progress**\n",
    "\n",
    "Implemented a Word2Vev model from scratch, largely following the tutorial linked below, although currently in progress expanding this to additional algorithms and areas of interest\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/word2vec\n",
    "\n",
    "TODO - describe vectorized embeddings\n",
    "\n",
    "For now, see this blog post - https://txt.cohere.com/sentence-word-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import File and show first 25 lines\n",
    "\n",
    "Load a file with about 6 or so novels from Project Gutenberg, the first book is Mary Shelley's Frankenstein as you can see from the preview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter 1\n",
      "\n",
      "_To Mrs. Saville, England._\n",
      "\n",
      "\n",
      "St. Petersburgh, Dec. 11th, 17—.\n",
      "\n",
      "\n",
      "You will rejoice to hear that no disaster has accompanied the\n",
      "commencement of an enterprise which you have regarded with such evil\n",
      "forebodings. I arrived here yesterday, and my first task is to assure\n",
      "my dear sister of my welfare and increasing confidence in the success\n",
      "of my undertaking.\n",
      "\n",
      "I am already far north of London, and as I walk in the streets of\n",
      "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
      "braces my nerves and fills me with delight. Do you understand this\n",
      "feeling? This breeze, which has travelled from the regions towards\n",
      "which I am advancing, gives me a foretaste of those icy climes.\n",
      "Inspirited by this wind of promise, my daydreams become more fervent\n",
      "and vivid. I try in vain to be persuaded that the pole is the seat of\n",
      "frost and desolation; it ever presents itself to my imagination as the\n",
      "region of beauty and delight. There, Margaret, the sun is for ever\n",
      "visible, its broad disk just skirting the horizon and diffusing a\n",
      "perpetual splendour. There—for with your leave, my sister, I will put\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "\n",
    "file_name = 'some-books-extended.txt'\n",
    "fullPath = os.path.abspath(\"./\" + file_name)\n",
    "path_to_file = tf.keras.utils.get_file(file_name, 'file://'+fullPath)\n",
    "\n",
    "with open(path_to_file) as f:\n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:25]:\n",
    "  print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty lines from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation characters and lowercase all the words\n",
    "\n",
    "Since individual words (tokens) will be used for word vectorization, we wouldn't want to calculate different vectors from \"the\" and \"The\".\n",
    "\n",
    "This function will be called below in the Text Vectorization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text\n",
    "\n",
    "The goal here is to translate words into indexed tokens which the vectors will represent.\n",
    "\n",
    "Note we are calling our standardization method above to pre-process the text.\n",
    "\n",
    "We trim the token size to a max vocabulary, this will remove the least frequently counted tokens.\n",
    "\n",
    "Note - we aren't computing anything yet. We are defining a keras layer in Tensor Flow\n",
    "\n",
    "## Note on changing to Ragged Output - see changes in output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 15\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    ragged=True,\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute a vocabulary of string terms (tokens)\n",
    "\n",
    "Utilizes the layer defined above\n",
    "\n",
    "From documentation:\n",
    "\n",
    "During adapt(), the layer will build a vocabulary of all string tokens seen in the dataset, sorted by occurrence count, with ties broken by sort order of the tokens (high to low). At the end of adapt(), if max_tokens is set, the vocabulary will be truncated to max_tokens size. For example, adapting a layer with max_tokens=1000 will compute the 1000 most frequent tokens occurring in the input dataset. If output_mode='tf-idf', adapt() will also learn the document frequencies of each token in the input dataset.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt\n",
    "\n",
    "\n",
    "We are referencing the text dataset created above in batches of 1024 characters at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:56:16.899146: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#vectorize_layer.adapt(text_ds.batch(1024))\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the computed vocabulary (tokens)\n",
    "\n",
    "Ultimately, when we are done, each token will have a multi-dimensional point (well really a vector in pure math terms) that represents it\n",
    "\n",
    "The array is called inverse vocab, since from this point on, we will reference a token by it's numerical ID, not it's string value, although it's position in the array is it's ID\n",
    "\n",
    "Ultimately, we will save off a .tsv file which is the textual metadata to accompany the array of N-Dimensional vectors we are aiming to produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'of', 'to', 'a', 'in', 'he', 'that', 'his', 'was', 'i', 'it', 'with', 'had', 'as', 'at', 'but', 'not']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace each word in the data set with its numerical index value\n",
    "\n",
    "If a data set is \"and then I saw the light and\"\n",
    "\n",
    "And we created a vectorized vocabulary where each word in the array, it's index is it's numerical value:\n",
    "[\"and\", \"then\", \"I\", \"saw\", \"the\", \"light\"]\n",
    "\n",
    "This step will translate the data set from the string to it's token index representation: \n",
    "\n",
    "[1,2,3,4,5,1]\n",
    "\n",
    "Note the use of prefetch and autotune, this is described in detail here and deals with the performance of the operation across a large data set - https://www.tensorflow.org/guide/data_performance\n",
    "\n",
    "Note also that our sequences were defined as length of 10 above, so note the difference in length between original lines of data and the transformed vectorized (and split to sequences of 10) data. \n",
    "\n",
    "**note** that how we create these sequences could be improved especially given that vectorizing is an attempt to capture semantic and contextual meaning of words, this is an area of possible improvement. Think about a sliding window for n-gram going across broken sentences.\n",
    "\n",
    "Also a note on the out of range error - https://stackoverflow.com/questions/53930242/how-to-fix-a-outofrangeerror-end-of-sequence-error-when-training-a-cnn-with-t\n",
    "\n",
    "This is expected behavior based on using a Tensorflow iterator instead of for loop with exact dimension, but it is expected behavior in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing Original Data\n",
      "tf.Tensor(b'Letter 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'_To Mrs. Saville, England._', shape=(), dtype=string)\n",
      "tf.Tensor(b'St. Petersburgh, Dec. 11th, 17\\xe2\\x80\\x94.', shape=(), dtype=string)\n",
      "tf.Tensor(b'You will rejoice to hear that no disaster has accompanied the', shape=(), dtype=string)\n",
      "tf.Tensor(b'commencement of an enterprise which you have regarded with such evil', shape=(), dtype=string)\n",
      "\n",
      "\n",
      "Showing Transformed Data\n",
      "tf.Tensor([ 384 2951], shape=(2,), dtype=int64)\n",
      "tf.Tensor([   5  769    1 1562], shape=(4,), dtype=int64)\n",
      "tf.Tensor([1404    1    1    1    1], shape=(5,), dtype=int64)\n",
      "tf.Tensor([  25   67    1    5  341    9   54    1  101 1443    2], shape=(11,), dtype=int64)\n",
      "tf.Tensor([4025    4   42 3595   38   25   36 1141   14   92 1241], shape=(11,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:56:16.981212: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-03-31 09:56:17.014679: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "# Vectorize the data in text_ds.\n",
    "#text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "text_vector_ds = text_ds.map(vectorize_layer)\n",
    "\n",
    "#show a snippet of the transformed data set\n",
    "print(\"Showing Original Data\")\n",
    "snippet_size=5\n",
    "snippet_original_data = text_ds.take(snippet_size)\n",
    "for data in snippet_original_data:\n",
    "    print(data)\n",
    "\n",
    "print(\"\\n\\nShowing Transformed Data\")\n",
    "snippet_data = text_vector_ds.take(snippet_size)\n",
    "for data in snippet_data:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the total number of sequences computed in the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:56:20.269342: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a breakdown again of token index to vectorized input\n",
    "\n",
    "Similar to what was printed above, another syntactical way to view a sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 384 2951] => ['letter', '1']\n",
      "[   5  769    1 1562] => ['to', 'mrs', '[UNK]', 'england']\n",
      "[1404    1    1    1    1] => ['st', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "[  25   67    1    5  341    9   54    1  101 1443    2] => ['you', 'will', '[UNK]', 'to', 'hear', 'that', 'no', '[UNK]', 'has', 'accompanied', 'the']\n",
      "[4025    4   42 3595   38   25   36 1141   14   92 1241] => ['commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sampling Data\n",
    "\n",
    "Below we are defining a method for generating sample training data, not yet calling it.\n",
    "\n",
    "OK, there's a lot going on here. When I have time I will summarize it better. \n",
    "\n",
    "The source tutorial has some good information - https://www.tensorflow.org/text/tutorials/word2vec#generate_training_data\n",
    "\n",
    "Something key to zoom in on that isn't described too well in the source tutorial - use of negative pairs.\n",
    "\n",
    "First we have to understand negative and positive sampling pairs. \n",
    "\n",
    "In the skip-gram technique, which is a type of word embedding model, the goal is to learn distributed representations of words in a continuous vector space. This is achieved by training a model to predict the context words given a target word, or vice versa. Skip-gram employs both positive and negative sampling to train the model efficiently on large datasets without requiring labeled data.\n",
    "Positive Sampling:\n",
    "\n",
    "    Definition: Positive sampling involves generating pairs of words where one word is the target word and the other is a context word that occurs within a certain window around the target word in the text corpus.\n",
    "    Example: If the sentence is \"The cat sat on the mat\", and we consider a window size of 2, then for the word \"cat\", positive samples might include pairs like (\"cat\", \"the\"), (\"cat\", \"sat\"), (\"cat\", \"on\"), and (\"cat\", \"mat\").\n",
    "    Training Objective: The neural network is trained to predict the context words given a target word. In other words, given the target word \"cat\", the network should predict \"the\", \"sat\", \"on\", and \"mat\".\n",
    "\n",
    "Negative Sampling:\n",
    "\n",
    "    Definition: Negative sampling addresses the imbalance between positive (context) and negative (non-context) examples by randomly selecting negative samples during training.\n",
    "    Example: For each positive skip-gram pair (\"cat\", \"the\"), instead of considering all non-context words as negative examples, negative sampling randomly selects a subset of words from the vocabulary that are not context words for \"cat\". These randomly selected words serve as negative examples.\n",
    "    Training Objective: The neural network is trained to distinguish between true context words (positive examples) and randomly sampled non-context words (negative examples). It learns to assign higher probabilities to true context words and lower probabilities to randomly sampled non-context words.\n",
    "\n",
    "**important note about how correct values are computed without labelled data**\n",
    "\n",
    "Skip-gram training is considered unsupervised or unlabeled training because it doesn't require labeled data. The model learns from the structure of the text corpus itself.\n",
    "During training, the model computes a loss function based on the predictions it makes for positive and negative samples.\n",
    "\n",
    "Importantly, random values are used to fill the array. These random values might indicate \"dog\" is semantically similar to \"computer\"\n",
    "\n",
    "Through iterations of training, via loss calculation, it refines these initial random guesses and derives true values across the matrices of data. IE model weights are created through simple mathematical calculations iteratively. I suggest learning more about this processs as it is a key aspect of understanding intuitively how LLMs are trained in the pre-training phase.\n",
    "\n",
    "Next we have to understand how the worst case would be computed to see why we are doing these negative approximations.\n",
    "\n",
    "In the worse case, for every positive word association, we would also train backpropogation in reverse every other token in the input text!\n",
    "\n",
    "The code below uses approximation of negative sampling based on research that shows picking a few negative examples at random is sufficient. \n",
    "\n",
    "The use of negative sampling in skip-gram models is a technique employed to address the imbalance between the number of negative examples and positive examples in the dataset. In skip-gram models, for each positive skip-gram pair (target word, context word), there are potentially a vast number of words in the vocabulary that are not context words for the target word. This leads to a highly imbalanced dataset, as the majority of training examples would be negative (non-context) examples.\n",
    "\n",
    "By using negative sampling, we can reduce the computational cost associated with training on this imbalanced dataset. Instead of considering all non-context words as negative examples, negative sampling randomly selects a small subset of negative examples for each positive example during training. This subset is typically much smaller than the entire vocabulary size, making the training process more efficient.\n",
    "\n",
    "Another note on negative sampling - \n",
    "\n",
    "I haven't gotten great results with these vectors. I added more books to the data set which improved results, I've tried tweaking parameters such as context window length for skip gram and things like that. I am currently thinking that the negative sampling method, while mathematically sound, isn't quite right in the implementaiton below. More details on that method which is approximated here, I think this is an important area of future improvement for this implementation - https://arxiv.org/pdf/1402.3722v1.pdf\n",
    "\n",
    "## Explanation of the output - Targets, Contexts, and Labels\n",
    "\n",
    "Wait I thought this was unlabeled? It is, each pair is however labelled as positive or negative as described below - \n",
    "\n",
    "1. Targets:\n",
    "\n",
    "    Definition: The targets represent the target words for which we are training the model to predict the context words.\n",
    "    Purpose:\n",
    "        Training Objective: Each element in the targets list represents a target word from the training data. During training, the skip-gram model aims to predict the context words surrounding each target word. For example, if the target word is \"cat\" in the sentence \"The cat sat on the mat\", the model should predict context words like \"the\", \"sat\", \"on\", and \"mat\".\n",
    "        Model Input: The target words serve as inputs to the skip-gram model. The model tries to learn meaningful word embeddings for each target word based on its context.\n",
    "\n",
    "2. Contexts:\n",
    "\n",
    "    Definition: The contexts represent the context words surrounding the target words in the training data.\n",
    "    Purpose:\n",
    "        Training Objective: Each element in the contexts list contains the context words (both positive and negative) associated with a target word. Positive context words are the actual words occurring in the vicinity of the target word in the training data, while negative context words are randomly sampled from the vocabulary.\n",
    "        Model Input: The context words, along with the target word, are provided as input to the skip-gram model. The model learns to predict these context words given the target word, thereby capturing the semantic meaning and relationships between words.\n",
    "\n",
    "3. Labels:\n",
    "\n",
    "    Definition: The labels represent the labels associated with each target-context pair, indicating whether a context word is a positive example (1) or a negative example (0).\n",
    "    Purpose:\n",
    "        Training Objective: Each element in the labels list indicates whether a context word is a positive example (a true context word) or a negative example (a non-context word randomly sampled from the vocabulary). During training, the skip-gram model aims to correctly classify these context words based on their relevance to the target word.\n",
    "        Loss Calculation: The labels are used to compute the loss function during training. The model's predictions (probabilities) are compared against these labels to compute the loss, which guides the model's parameter updates through backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the method above to generate our training data\n",
    "\n",
    "Note a few parameters being set though - \n",
    "\n",
    "We set window size, which indicates how many context words are generated for each target, which informs the number of pairs created for training for positive correlations for each pair\n",
    "\n",
    "num_ns = Number of negative samples, see markdown above\n",
    "\n",
    "random seed - this random seed was set above, although it dictates the randomness used for the first initialization of the vector arrays, to better understand this see backprapogation training - https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd \n",
    "\n",
    "TODO - more simple example of backpropagation \n",
    "\n",
    "When we print the shape we are showing the legnth of each, which represents the number of training examples generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/91985 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91985/91985 [00:38<00:00, 2378.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (508793,)\n",
      "contexts.shape: (508793, 9)\n",
      "labels.shape: (508793, 9)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=3,\n",
    "    num_ns=8,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n",
    "\n",
    "#TODO count how many are positive etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO explanation about tensors\n",
    "\n",
    "## TODO revisit commented out shuffle and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 9), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 9), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO revisit caching and prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print a sample of the final training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of elements to print\n",
    "#TODO improve this a bit\n",
    "\n",
    "#num_elements_to_print = 2\n",
    "\n",
    "# Iterate over the dataset and print elements\n",
    "#for idx, ((target_batch, context_batch), label_batch) in enumerate(dataset.take(num_elements_to_print)):\n",
    " #   print(f\"Batch {idx + 1}:\")\n",
    "#    # Iterate over each example within the batch\n",
    "  #  for i in range(target_batch.shape[0]):\n",
    " #       print(f\"Example {i + 1}:\")\n",
    "  #      print(\"Target:\", target_batch[i].numpy())\n",
    " #       print(\"Context:\", context_batch[i].numpy())\n",
    "  #      print(\"Label:\", label_batch[i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class wrapper for the model\n",
    "\n",
    "TODO - add detail here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "TODO - explanation of batch and epoch size vs total number of training pairs above\n",
    "\n",
    "TODO - improve to fully use training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "## TRAIN MODEL!\n",
    "word2vec.fit(dataset, epochs=80, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at some of the vectors\n",
    "\n",
    "TODO - describe in layman's terms what a vector is and why it's useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.3750799e-02 -2.7074946e-02 -2.6434351e-02  1.8690120e-02\n",
      " -3.2399438e-02  2.0251084e-02 -5.0881989e-03 -6.5543167e-03\n",
      "  3.0071251e-03  3.0471098e-02  2.1930788e-02 -4.3657400e-02\n",
      " -1.6496133e-02 -1.0000370e-02 -1.4389634e-02 -4.8357189e-02\n",
      "  1.0558821e-02  4.1770961e-02  2.1929298e-02 -2.3934985e-02\n",
      "  3.7337806e-02 -3.0260289e-02  2.3448896e-02  3.1434331e-02\n",
      "  3.9561238e-02 -4.1341115e-02 -2.2916198e-02 -5.1372051e-03\n",
      "  2.7572561e-02  3.2598328e-02  3.5328437e-02 -4.5892444e-02\n",
      " -1.4512420e-02 -3.3295415e-02 -1.8995596e-02 -1.7882574e-02\n",
      " -1.6142618e-02  3.8369145e-02  3.3047486e-02  2.2687018e-05\n",
      "  4.2885493e-02  1.6078521e-02  4.4992756e-02  4.9504153e-03\n",
      " -1.6119875e-02 -2.4573540e-02  1.6998500e-05 -3.6284495e-02\n",
      " -2.1811016e-03  1.4689755e-02 -9.8945983e-03 -1.0446571e-02\n",
      " -8.5941069e-03 -3.7145913e-02 -3.2512225e-02 -2.5105810e-02\n",
      " -1.4876522e-02 -4.2062949e-02  2.8081920e-02  4.6249632e-02\n",
      "  6.1083920e-03  1.1597395e-02  4.8862759e-02 -2.4621129e-02]\n",
      "[-0.21858142 -0.59042734 -0.20193322 -0.84620255  0.48735324  0.01806597\n",
      " -0.8051547   0.37381312  0.77125853 -0.60407966  0.19638513  0.58866763\n",
      "  0.5547539   0.02678289 -0.17018095 -0.5897392  -0.03758833  1.1106253\n",
      "  0.3792177   0.1285829  -0.37649775  0.00970701  0.25855675 -0.63282293\n",
      "  1.2099994  -0.22137107 -0.49334356 -0.70162135 -0.03853506  0.66588545\n",
      " -0.7059113   0.49122566  0.11147121 -0.8234063  -0.39480725 -0.41666013\n",
      " -0.09136187  0.1954871   0.11157785 -0.24629879  0.20759638 -0.60019416\n",
      "  0.26434198  0.18171915  0.51662934  0.7221044  -0.33962512  0.16937447\n",
      " -0.8498696   0.1859673  -0.1248598   0.44956496  0.20890573 -0.4641816\n",
      "  0.22614163 -1.0612997  -0.82505053 -0.42379442 -0.4150684   0.1898801\n",
      "  0.54468536 -0.18846823  0.26319236 -0.03931723]\n"
     ]
    }
   ],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "print(weights[0])\n",
    "print(weights[467])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'of', 'to', 'a', 'in', 'he', 'that', 'his', 'was', 'i', 'it', 'with', 'had', 'as', 'at', 'but', 'not', 'for', 'him', 'on', 'her', 'is', 'you', 'all', 'she', 'from', 'by', 'be', 'this', 'my', 'were', 'said', 'they', 'have', 'one', 'which', 'so', 'me', 'what', 'an', 'who', 'when', 'or', 'there', 'their', 'up', 'been', 'would', 'them', 'now', 'out', 'no', 'if', 'are', 'did', 'into', 'only', 'more', 'then', 'we', 'could', 'some', 'prince', 'man', 'will', 'pierre', 'about', 'like', 'time', 'do', 'before', 'your', 'very', 'old', 'himself', 'after', 'over', 'how', 'down', '“i', 'other', 'see', 'these', 'eyes', 'know', 'again', 'its', 'still', 'went', 'such', 'little', 'thought', 'than', 'first', 'any', 'those', 'came', 'face', 'has', 'men', 'two', 'seemed', 'room', 'our', 'natásha', 'long', 'go', 'andrew', 'without', 'even', 'same', 'head', 'say', 'must', 'where', 'way', 'away']\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(vocab[:120])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the vocabulary and model weights (vectors) to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('book-vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('book-metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the loss propagation in a Tensorboard Magic Cell\n",
    "\n",
    "TODO - describe why more epochs didn't yeild better results\n",
    "\n",
    "Photo of result:\n",
    "\n",
    "![Tensor Board Example](tensorboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - implement cosine-similarity and find most similar words here\n",
    "\n",
    "For now, load the weights here - https://projector.tensorflow.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def calcCosineSimilarityOfWords(targetWord, comparisonWord):\n",
    "    #Get the indexes of both words\n",
    "    targetWordIndex = vocab.index(targetWord)\n",
    "    comparisonWordIndex = vocab.index(comparisonWord)\n",
    "\n",
    "    # Get the vectors for both words\n",
    "    targetWordVector = weights[targetWordIndex]\n",
    "    comparisonWordVector = weights[comparisonWordIndex]\n",
    "\n",
    "    #Compute Cosine Similarity\n",
    "    cosine = np.dot(targetWordVector,comparisonWordVector)/(norm(targetWordVector)*norm(comparisonWordVector))\n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american, vast:  0.44785282\n",
      "american, europe:  0.43727928\n",
      "american, consider:  0.08188085\n"
     ]
    }
   ],
   "source": [
    "print(\"american, vast: \", calcCosineSimilarityOfWords(\"american\", \"vast\"))\n",
    "print(\"american, europe: \", calcCosineSimilarityOfWords(\"american\", \"europe\"))\n",
    "print(\"american, consider: \", calcCosineSimilarityOfWords(\"american\", \"consider\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
